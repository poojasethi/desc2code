A computer has a dual core processor. Its operating system has a set of jobs JS from an application that it has to execute. A job scheduled to a core runs on that core till its completion. The scheduler is optimized to schedule jobs to different cores such that at the end of execution of JS, the skew of the processing time between both the cores is minimum. For example if there are three jobs A, B and C requiring 2, 3 and 4 milliseconds each respectively, then the scheduler will assign the jobs A and B to Core 1 and job C to Core 2. In this case the skew is |(2+3) - 4| = 1. Yet another scheduling policy can be to assign job A to Core 1 and jobs B and C on Core 2 but in this case the skew will be |2 - (3+4)| = 5, which is not optimal because Core 1 will finish its job in 2 milliseconds and keep sitting idle for five extra milliseconds while Core 2 finishes its jobs in 7 milliseconds. Your task is to find the skew assuming that 

the scheduler optimally schedules the jobs to different cores
no other job gets allocated a time quantum while any job from JS is executing
scheduling policy is non-preemptive


Input
First line of the input is the number of test cases T, after which T test cases follow. Each test case begins with an integer n in a line, i.e. the number of jobs in JS; followed by burst times(in milliseconds) required for each of the n jobs in a single line with each value separated by single space.


Output
For each test case output a single integer s, the skew for that test case; followed by a new line character.

Constraints

1<= t <= 10
1<= n <= 1000


Example

Input:
2
5
2 3 4 6 8
1
6

Output:
1
6  
